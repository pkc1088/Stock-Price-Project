{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a17da1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "import re\n",
    "\n",
    "# marketaux API-Key\n",
    "MARKETAUX_API_KEY = \"hMCAuYSbfahDopkGP1DmoEZdn90ky3LPWL6kJzoq\"\n",
    "# Gemini API-Key\n",
    "API_KEY = \"AIzaSyAKUehrFeC1tiZG8swNGO94VI4q3Fer0Bw\"\n",
    "# NewsAPI\n",
    "News_API_KEY = \"97f1e601562e49ccbe6b33f2dcf1ebc3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbf52b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBC\n",
    "def extract_bbc_article_text(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    main_content = soup.find('main', id='main-content')\n",
    "    if not main_content:\n",
    "        print(\"main-content를 찾을 수 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    article = main_content.find('article')\n",
    "    if not article:\n",
    "        print(\"article을 찾을 수 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    text_blocks = article.find_all('div', attrs={'data-component': 'text-block'})\n",
    "    if not text_blocks:\n",
    "        print(\"text-block을 찾을 수 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    paragraphs = []\n",
    "    for block in text_blocks:\n",
    "        p_tags = block.find_all('p')\n",
    "        for p in p_tags:\n",
    "            text = p.get_text(strip=True)\n",
    "            if text: \n",
    "                paragraphs.append(text)\n",
    "\n",
    "    if not paragraphs:\n",
    "        print(\"본문을 찾을 수 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    full_text = \"\\n\".join(paragraphs)\n",
    "    return full_text\n",
    "\n",
    "\n",
    "# CBS\n",
    "def extract_cbs_news_text(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    article_section = soup.select_one(\"article section.content__body\")\n",
    "\n",
    "    if not article_section:\n",
    "        print(\"본문 섹션을 찾을 수 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    paragraphs = article_section.find_all(\"p\")\n",
    "    article_text = \"\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "\n",
    "    return article_text\n",
    "\n",
    "\n",
    "# CNN\n",
    "def extract_cnn_news_text(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        article_root = soup.select_one(\"body div section article[data-uri] section main\")\n",
    "        if not article_root:\n",
    "            print(\"본문 루트(main)를 찾지 못했습니다.\")\n",
    "            return None\n",
    "\n",
    "        paragraphs = article_root.find_all(\"p\")\n",
    "        article_text = \"\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "\n",
    "        return article_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Yahoo\n",
    "def extract_yahoo_finance_article_text(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    svelte_div = soup.find('div', id='svelte')\n",
    "    if not svelte_div:\n",
    "        print(\"div#svelte tag error\")\n",
    "        return None\n",
    "\n",
    "    main_tag = svelte_div.find('main')\n",
    "    if not main_tag:\n",
    "        print(\"<main> tag error\")\n",
    "        return None\n",
    "\n",
    "    section_tag = main_tag.find('section')\n",
    "    if not section_tag:\n",
    "        print(\"<section> tag error\")\n",
    "        return None\n",
    "\n",
    "    paragraphs = []\n",
    "    for p in section_tag.find_all('p'):\n",
    "        text = p.get_text(strip=True)\n",
    "        if text:\n",
    "            paragraphs.append(text)\n",
    "\n",
    "    if not paragraphs:\n",
    "        print(\"<p> tag error\")\n",
    "        return None\n",
    "\n",
    "    return '\\n'.join(paragraphs)\n",
    "\n",
    "\n",
    "# URL Adapter\n",
    "def extract_article_text(url):\n",
    "    url = url.lower() \n",
    "\n",
    "    if 'bbc' in url:\n",
    "        return extract_bbc_article_text(url)\n",
    "    elif 'yahoo' in url:\n",
    "        return extract_yahoo_finance_article_text(url)\n",
    "    elif 'cbsnews' in url:\n",
    "        return extract_cbs_news_text(url)\n",
    "    elif 'cnn' in url:\n",
    "        return extract_cnn_news_text(url)\n",
    "    else:\n",
    "        print(\"URL support error\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "# text parser\n",
    "def collect_all_news_texts(url_list):\n",
    "    all_texts = []\n",
    "\n",
    "    for url in url_list:\n",
    "        text = extract_article_text(url)\n",
    "        if text:\n",
    "            all_texts.append(text.strip())\n",
    "\n",
    "    return \"\\n\\n\".join(all_texts)\n",
    "    \n",
    "\n",
    "# Yahoo Dynamic Crawling\n",
    "def extract_yahoo_finance_news_links(ticker_url):\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  \n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(ticker_url)\n",
    "\n",
    "    # 페이지 로딩 대기\n",
    "    time.sleep(5) \n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    news_panel = soup.find('div', id='tabpanel-news')\n",
    "    if not news_panel:\n",
    "        print(\"tabpanel-news를 찾을 수 없습니다.\")\n",
    "        driver.quit()\n",
    "        return []\n",
    "\n",
    "    links = []\n",
    "\n",
    "    stream_items = news_panel.find_all('div', class_=['stream-item', 'yf-186c5b2'])\n",
    "\n",
    "    for item in stream_items:\n",
    "        section = item.find('section')\n",
    "        if section:\n",
    "            a_tag = section.find('a', class_='subtle-link', href=True)\n",
    "            if a_tag:\n",
    "                links.append(a_tag['href'])\n",
    "\n",
    "    driver.quit()\n",
    "    return links\n",
    "\n",
    "\n",
    "# Marketaux API\n",
    "def fetch_sentiments(symbol):\n",
    "    marketaux_url = \"https://api.marketaux.com/v1/news/all\"\n",
    "    params = {\n",
    "        \"symbols\": symbol,\n",
    "        \"filter_entities\": \"true\",\n",
    "        \"published_after\": \"2025-04-22\",\n",
    "        \"limit\": 3,\n",
    "        \"language\": \"en\",\n",
    "        \"api_token\": MARKETAUX_API_KEY,\n",
    "    }\n",
    "\n",
    "    response = requests.get(marketaux_url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    marketaux_data = response.json()\n",
    "    sentiments = []\n",
    "\n",
    "    for article in marketaux_data.get('data', []):\n",
    "        for entity in article.get('entities', []):\n",
    "            if entity.get('sentiment_score') is not None:\n",
    "                sentiments.append(entity['sentiment_score'])\n",
    "\n",
    "    return sentiments\n",
    "\n",
    "\n",
    "#  NewsAPI\n",
    "def fetch_news_from_newsapi(query, from_date, sources, api_key, page_size=10):\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"from\": from_date,\n",
    "        \"sources\": \",\".join(sources),\n",
    "        \"apiKey\": api_key,\n",
    "        \"language\": \"en\",\n",
    "        # \"pageSize\": page_size\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"NewsAPI request failed: {response.status_code} - {response.text}\")\n",
    "\n",
    "    data = response.json()\n",
    "    urls = [article['url'] for article in data.get('articles', [])]\n",
    "    return urls\n",
    "\n",
    "\n",
    "# Gemini API\n",
    "def analyze_sentiment_with_gemini(news_text, stock_name):\n",
    "    \n",
    "    genai.configure(api_key=API_KEY)\n",
    "    model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "\n",
    "    combined_news = \"\\n\\n\".join(news_text)\n",
    "\n",
    "    prompt = (\n",
    "        f\"Read the following news reports about {stock_name} \"\n",
    "        \"and judge the overall sentiment between -1 (very negative) and 1 (very positive).\\n\"\n",
    "        \"Respond strictly in the following format:\\n\"\n",
    "        \"Line 1: A single float number representing the sentiment score (e.g., 0.35)\\n\"\n",
    "        \"Line 2: A short explanation (1-3 sentences) explaining why you gave that score.\\n\"\n",
    "        \"Do not include any headings, markdown, or extra formatting.\\n\\n\"\n",
    "        f\"News:\\n{combined_news}\"\n",
    "    )\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "    lines = response.text.strip().split('\\n')\n",
    "\n",
    "    score = None\n",
    "    explanation = \"\"\n",
    "    if len(lines) >= 2:\n",
    "        match = re.match(r\"^(-?\\d+(\\.\\d+)?)$\", lines[0].strip())\n",
    "        if match:\n",
    "            try:\n",
    "                score = float(match.group(0))\n",
    "            except ValueError:\n",
    "                raise ValueError(\"Invalid score format returned.\")\n",
    "        else:\n",
    "            raise ValueError(\"Sentiment score not found in the first line.\")\n",
    "        explanation = \"\\n\".join(lines[1:]).strip()\n",
    "    else:\n",
    "        raise ValueError(\"Response does not contain enough lines.\")\n",
    "\n",
    "    return score, explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/get-news', methods=['POST'])\n",
    "def get_news():\n",
    "    req_data = request.get_json()\n",
    "    symbol = req_data.get('symbol')\n",
    "    ticker_name = req_data.get('ticker_name')\n",
    "\n",
    "    if not symbol or not ticker_name:\n",
    "        return jsonify({\"error\": \"Both 'symbol' and 'ticker_name' are required.\"}), 400\n",
    "\n",
    "    try:\n",
    "        from_date = \"2025-05-02\"\n",
    "        sources = [\"cbs-news\", \"bbc-news\", \"cnn\"]\n",
    "\n",
    "        # NewsAPI 뉴스\n",
    "        news_url_lists = fetch_news_from_newsapi(ticker_name, from_date, sources, News_API_KEY)\n",
    "\n",
    "        # Yahoo Finance 뉴스 링크 추가\n",
    "        yahoo_url = f\"https://finance.yahoo.com/quote/{symbol}\"\n",
    "        yahoo_news_links = extract_yahoo_finance_news_links(yahoo_url)\n",
    "        news_url_lists.extend(yahoo_news_links)\n",
    "\n",
    "\n",
    "\n",
    "        # 뉴스 본문 텍스트 수집\n",
    "        full_news_text = collect_all_news_texts(news_url_lists)\n",
    "\n",
    "        # Gemini 호출\n",
    "        score, explanation = analyze_sentiment_with_gemini(news_text=full_news_text, stock_name=ticker_name)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "    \n",
    "    sentiments = fetch_sentiments(symbol)\n",
    "    if sentiments is None:\n",
    "        return jsonify({\"error\": \"Failed to fetch data from Marketaux\"}), 5\n",
    "    \n",
    "    return jsonify({\"symbol\": symbol, \"ticker_name\": ticker_name, \"sentiments\": sentiments, \"score\": score, \"explanation\": explanation})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b624c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/get-news', methods=['POST'])\n",
    "def get_news():\n",
    "    req_data = request.get_json()\n",
    "    symbol = req_data.get('symbol')\n",
    "    ticker_name = req_data.get('ticker_name')\n",
    "\n",
    "    if not symbol or not ticker_name:\n",
    "        return jsonify({\"error\": \"Both 'symbol' and 'ticker_name' are required.\"}), 400\n",
    "\n",
    "    try:\n",
    "        from_date = \"2025-05-04\"\n",
    "        sources = [\"cbs-news\", \"bbc-news\", \"cnn\"]\n",
    "\n",
    "        # NewsAPI \n",
    "        news_url_lists = fetch_news_from_newsapi(ticker_name, from_date, sources, News_API_KEY)\n",
    "        print(f\"[INFO] NewsAPI에서 가져온 URL 개수: {len(news_url_lists)}\")\n",
    "\n",
    "        # Yahoo Finance URL crawling\n",
    "        yahoo_url = f\"https://finance.yahoo.com/quote/{symbol}\"\n",
    "        yahoo_news_links = extract_yahoo_finance_news_links(yahoo_url)\n",
    "        print(f\"[INFO] Yahoo Finance에서 가져온 뉴스 링크 개수: {len(yahoo_news_links)}\")\n",
    "\n",
    "        news_url_lists.extend(yahoo_news_links)\n",
    "        print(f\"[INFO] 전체 뉴스 URL 개수 (합산): {len(news_url_lists)}\")\n",
    "        \n",
    "        # News full contents\n",
    "        full_news_text = collect_all_news_texts(news_url_lists)\n",
    "        print(f\"[INFO] 수집한 뉴스 본문 전체 길이: {len(full_news_text)}자\")\n",
    "\n",
    "        # Gemini \n",
    "        score, explanation = analyze_sentiment_with_gemini(news_text=full_news_text, stock_name=ticker_name)\n",
    "        print(f\"[INFO] Gemini 분석 결과 - Score: {score}, Explanation: {explanation}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] 예외 발생: {str(e)}\")\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "    # Marketaux\n",
    "    sentiments = fetch_sentiments(symbol)\n",
    "    if sentiments is None:\n",
    "        print(\"[ERROR] Marketaux 감정 점수 가져오기 실패\")\n",
    "        return jsonify({\"error\": \"Failed to fetch data from Marketaux\"}), 500\n",
    "\n",
    "    return jsonify({\n",
    "        \"symbol\": symbol,\n",
    "        \"ticker_name\": ticker_name,\n",
    "        \"sentiments\": sentiments,\n",
    "        \"score\": score,\n",
    "        \"explanation\": explanation\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
