{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c15d765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from CNN_PY import cnn_model\n",
    "from LSTM_PY import lstm_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import keras\n",
    "import joblib\n",
    "import ta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from PIL import Image\n",
    "import mplfinance as mpf\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "# def create_meta_fusion_model():\n",
    "#     # 입력: [LSTM 변화율, CNN 상승 확률, CNN 하락 확률]\n",
    "#     inputs = layers.Input(shape=(3,))  # 입력 피처 3개\n",
    "#     x = layers.Dense(32, activation='relu')(inputs)  # 은닉층 32 노드\n",
    "#     x = layers.Dense(16, activation='relu')(x)       # 은닉층 16 노드\n",
    "#     output = layers.Dense(1, activation='tanh')(x)   # 출력층: -1 ~ 1 (하락 ~ 상승)\n",
    "    \n",
    "#     model = models.Model(inputs, output)\n",
    "#     return model\n",
    "\n",
    "# meta_model = create_meta_fusion_model()\n",
    "# meta_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# cnn_model('2025-05-12', 'AAPL')\n",
    "# lstm_model_1 = tf.keras.layers.TFSMLayer('LSTM_MODEL_TF_GPU/AOS', call_endpoint='serving_default')\n",
    "# lstm_mode_2 = tf.keras.models.load_model(os.path.join('LSTM_MODEL_H5_WIN', 'AOS.h5'), compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37f410c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
      "AAPL\n",
      "2024-05-14\n",
      "180.89638493603647\n",
      "neutral\n",
      "0.9252076745033264\n",
      "111\n"
     ]
    }
   ],
   "source": [
    "symbol = 'AAPL'\n",
    "date = '2024-05-14'\n",
    "close = lstm_model(date, symbol)\n",
    "cnn_output = cnn_model(date, symbol)\n",
    "label = cnn_output[0]\n",
    "prob = float(cnn_output[1])\n",
    "actual_close =  111 #yfinance에서 받아와야함\n",
    "print(symbol)\n",
    "print(date)\n",
    "print(close)\n",
    "print(label)\n",
    "print(prob)\n",
    "print(actual_close)\n",
    "file_path = 'MLP_sp500.csv'\n",
    "\n",
    "with open(file_path, 'a', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([symbol, date, close, label, prob, actual_close])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "041073c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MLP_sp500.csv 초기화 완료 (Close 비워지고, Actual_Close만 채워짐)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 전체 S&P500 최신 데이터 로딩\n",
    "df_all = pd.read_csv('sp500_latest.csv', parse_dates=['Date'])\n",
    "\n",
    "# 원하는 기간 필터 (2024-01-01 ~ 2024-12-31)\n",
    "df_filtered = df_all[(df_all['Date'] >= '2024-01-01') & (df_all['Date'] < '2025-01-01')]\n",
    "\n",
    "# 필요한 칼럼 생성 및 초기화\n",
    "df_mlp_init = df_filtered[['Symbol', 'Date']].copy()\n",
    "df_mlp_init['Close'] = ''  # 예측값 저장용 → 초기 빈값\n",
    "df_mlp_init['Label'] = ''  # 예측 Label → 빈값\n",
    "df_mlp_init['Prob'] = ''   # 예측 확률 → 빈값\n",
    "df_mlp_init['Actual_Close'] = df_filtered['Close']  # 실제 종가만 채워 넣음\n",
    "\n",
    "# 저장\n",
    "df_mlp_init.to_csv('MLP_sp500.csv', index=False)\n",
    "print(\"✅ MLP_sp500.csv 초기화 완료 (Close 비워지고, Actual_Close만 채워짐)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52cc683c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "종목 처리 중:   0%|          | 0/497 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step\n",
      "CNN 예측 오류 (MMM): Data for column \"Open\" must be ALL float or int.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
      "CNN 예측 오류 (MMM): Data for column \"Open\" must be ALL float or int.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
      "CNN 예측 오류 (MMM): Data for column \"Open\" must be ALL float or int.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step\n",
      "CNN 예측 오류 (MMM): Data for column \"Open\" must be ALL float or int.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000158A5DF2FC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000158A5DF2FC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step\n",
      "CNN 예측 오류 (MMM): Data for column \"Open\" must be ALL float or int.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000158A5EB2480> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000158A5EB2480> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step\n",
      "CNN 예측 오류 (MMM): Data for column \"Open\" must be ALL float or int.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n",
      "CNN 예측 오류 (MMM): Data for column \"Open\" must be ALL float or int.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step\n",
      "CNN 예측 오류 (MMM): Data for column \"Open\" must be ALL float or int.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step\n",
      "CNN 예측 오류 (MMM): Data for column \"Open\" must be ALL float or int.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
      "CNN 예측 오류 (MMM): Data for column \"Open\" must be ALL float or int.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "종목 처리 중:   0%|          | 0/497 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN 예측 오류 (MMM): Data for column \"Open\" must be ALL float or int.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# LSTM 예측 (전체 데이터 전달)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m close_pred = \u001b[43mlstm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m close_pred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\Desktop\\Graduation Project\\LSTM_PY.py:62\u001b[39m, in \u001b[36mlstm_model\u001b[39m\u001b[34m(target_date_str, ticker, full_df)\u001b[39m\n\u001b[32m     59\u001b[39m X_scaled = scaler.transform(df.iloc[end_idx-\u001b[32m49\u001b[39m:end_idx+\u001b[32m1\u001b[39m][features])\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# 예측\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m pred = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m dummy = np.zeros((\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(features)))\n\u001b[32m     64\u001b[39m dummy[:, features.index(\u001b[33m'\u001b[39m\u001b[33mClose\u001b[39m\u001b[33m'\u001b[39m)] = pred.flatten()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:557\u001b[39m, in \u001b[36mTensorFlowTrainer.predict\u001b[39m\u001b[34m(self, x, batch_size, verbose, steps, callbacks)\u001b[39m\n\u001b[32m    555\u001b[39m outputs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator.catch_stop_iteration():\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_predict_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:734\u001b[39m, in \u001b[36mTFEpochIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    733\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_epoch_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:102\u001b[39m, in \u001b[36mEpochIterator._enumerate_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m steps_per_epoch > \u001b[32m0\u001b[39m:\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._current_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps_per_epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m         \u001b[38;5;28mself\u001b[39m._current_iterator = \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m         \u001b[38;5;28mself\u001b[39m._steps_seen = \u001b[32m0\u001b[39m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, steps_per_epoch, \u001b[38;5;28mself\u001b[39m.steps_per_execution):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:501\u001b[39m, in \u001b[36mDatasetV2.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context.executing_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops.inside_function():\n\u001b[32m    500\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(\u001b[38;5;28mself\u001b[39m._variant_tensor):\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    503\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33miteration in eager mode or within tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:709\u001b[39m, in \u001b[36mOwnedIterator.__init__\u001b[39m\u001b[34m(self, dataset, components, element_spec)\u001b[39m\n\u001b[32m    705\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    707\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    708\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnot be specified.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m709\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[38;5;28mself\u001b[39m._get_next_call_count = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:748\u001b[39m, in \u001b[36mOwnedIterator._create_iterator\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    745\u001b[39m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype.args[\u001b[32m0\u001b[39m].args[\u001b[32m0\u001b[39m].args) == \u001b[38;5;28mlen\u001b[39m(\n\u001b[32m    746\u001b[39m       \u001b[38;5;28mself\u001b[39m._flat_output_types)\n\u001b[32m    747\u001b[39m   \u001b[38;5;28mself\u001b[39m._iterator_resource.op.experimental_set_type(fulltype)\n\u001b[32m--> \u001b[39m\u001b[32m748\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3509\u001b[39m, in \u001b[36mmake_iterator\u001b[39m\u001b[34m(dataset, iterator, name)\u001b[39m\n\u001b[32m   3507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m   3508\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3509\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3510\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMakeIterator\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   3512\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# multi_modal.ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from CNN_PY import cnn_model\n",
    "from LSTM_PY import lstm_model\n",
    "\n",
    "# 1. 공통 설정\n",
    "SYMBOLS_FILE = 'completed_symbols_h5.txt'\n",
    "OUTPUT_CSV = 'MLP_sp500_temp.csv'\n",
    "START_DATE = '2024-01-01'\n",
    "END_DATE = '2025-01-01'\n",
    "\n",
    "# 2. 종목 리스트 로드\n",
    "with open(SYMBOLS_FILE, 'r') as f:\n",
    "    symbols = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# 3. 기존 데이터 로드\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    mlp_df = pd.read_csv(OUTPUT_CSV)\n",
    "    existing_keys = set(zip(mlp_df['Symbol'], mlp_df['Date']))\n",
    "else:\n",
    "    mlp_df = pd.DataFrame(columns=['Symbol','Date','Close','Label','Prob','Actual_Close'])\n",
    "    existing_keys = set()\n",
    "\n",
    "# 4. 종목별 처리\n",
    "for symbol in tqdm(symbols, desc=\"종목 처리 중\"):\n",
    "    try:\n",
    "        # 1회만 데이터 다운로드 (추가 40일 버퍼 포함)\n",
    "        full_df = yf.download(\n",
    "            symbol, \n",
    "            start=pd.to_datetime(START_DATE)-timedelta(days=40),\n",
    "            end=pd.to_datetime(END_DATE)+timedelta(days=2),\n",
    "            progress=False\n",
    "        )\n",
    "        \n",
    "        if full_df.empty:\n",
    "            continue\n",
    "            \n",
    "        # 컬럼 정리\n",
    "        full_df = full_df[['Open','High','Low','Close','Volume']].dropna()\n",
    "        full_df['Date_Str'] = full_df.index.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # 대상 기간 필터링 (2024-01-01 ~ 2025-01-01)\n",
    "        target_df = full_df.loc[START_DATE:END_DATE].copy()\n",
    "        \n",
    "        for date_str, row in target_df.iterrows():\n",
    "            date_str = date_str.strftime('%Y-%m-%d')\n",
    "            \n",
    "            # 중복 체크\n",
    "            if (symbol, date_str) in existing_keys:\n",
    "                continue\n",
    "                \n",
    "            # LSTM 예측 (전체 데이터 전달)\n",
    "            close_pred = lstm_model(date_str, symbol, full_df)\n",
    "            if close_pred is None:\n",
    "                continue\n",
    "                \n",
    "            # CNN 예측 (전체 데이터 전달)\n",
    "            label, prob = cnn_model(date_str, symbol, full_df)\n",
    "            if label is None:\n",
    "                continue\n",
    "                \n",
    "            # 결과 저장\n",
    "            new_row = {\n",
    "                'Symbol': symbol,\n",
    "                'Date': date_str,\n",
    "                'Close': close_pred,\n",
    "                'Label': label,\n",
    "                'Prob': prob,\n",
    "                'Actual_Close': row['Close']\n",
    "            }\n",
    "            mlp_df = pd.concat([mlp_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            existing_keys.add((symbol, date_str))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n{symbol} 처리 실패: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# 5. 결과 저장\n",
    "mlp_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"\\n처리 완료! 총 {len(mlp_df)}개 데이터 저장됨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d99fe00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### MMM 처리 시작 ###\n",
      "MMM 2024-01-02: 이미 존재, 스킵\n",
      "MMM 2024-01-03: 이미 존재, 스킵\n",
      "MMM 2024-01-04: 이미 존재, 스킵\n",
      "MMM 2024-01-05: 이미 존재, 스킵\n",
      "MMM 2024-01-08: 이미 존재, 스킵\n",
      "MMM 2024-01-09: 이미 존재, 스킵\n",
      "MMM 2024-01-10: 이미 존재, 스킵\n",
      "MMM 2024-01-11: 이미 존재, 스킵\n",
      "MMM 2024-01-12: 이미 존재, 스킵\n",
      "MMM 2024-01-16: 이미 존재, 스킵\n",
      "MMM 2024-01-17: 이미 존재, 스킵\n",
      "MMM 2024-01-18: 이미 존재, 스킵\n",
      "MMM 2024-01-19: 이미 존재, 스킵\n",
      "MMM 2024-01-22: 이미 존재, 스킵\n",
      "MMM 2024-01-23: 이미 존재, 스킵\n",
      "MMM 2024-01-24: 이미 존재, 스킵\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMM | 2024-01-25 | Close(LSTM): 80.45 | Label: drop | Prob: 0.9585 | Actual_Close: 77.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "MMM | 2024-01-26 | Close(LSTM): 79.27 | Label: surge | Prob: 0.6807 | Actual_Close: 77.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMM | 2024-01-29 | Close(LSTM): 78.44 | Label: surge | Prob: 0.5555 | Actual_Close: 77.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMM | 2024-01-30 | Close(LSTM): 77.87 | Label: neutral | Prob: 0.8980 | Actual_Close: 77.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMM | 2024-01-31 | Close(LSTM): 77.34 | Label: neutral | Prob: 0.8891 | Actual_Close: 75.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMM | 2024-02-01 | Close(LSTM): 77.09 | Label: neutral | Prob: 0.6954 | Actual_Close: 77.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMM | 2024-02-02 | Close(LSTM): 76.97 | Label: surge | Prob: 0.5678 | Actual_Close: 76.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMM | 2024-02-05 | Close(LSTM): 76.76 | Label: neutral | Prob: 0.8827 | Actual_Close: 74.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMM | 2024-02-06 | Close(LSTM): 76.62 | Label: drop | Prob: 0.6132 | Actual_Close: 75.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMM | 2024-02-07 | Close(LSTM): 76.56 | Label: surge | Prob: 0.7602 | Actual_Close: 75.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# CNN 예측 Label, Prob\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m label, prob = \u001b[43mcnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m prob \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     71\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: CNN 예측 실패\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\Desktop\\Graduation Project\\CNN_PY.py:72\u001b[39m, in \u001b[36mcnn_model\u001b[39m\u001b[34m(target_date_str, ticker)\u001b[39m\n\u001b[32m     68\u001b[39m N_CANDLES = \u001b[32m20\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# print(f\"\\n{ticker} 데이터 다운로드 중...\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m     df = yf.download(\n\u001b[32m     74\u001b[39m         ticker,\n\u001b[32m     75\u001b[39m         start=target_date - timedelta(days=\u001b[32m40\u001b[39m),\n\u001b[32m     76\u001b[39m         end=target_date + timedelta(days=\u001b[32m2\u001b[39m),\n\u001b[32m     77\u001b[39m         group_by=\u001b[33m'\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     78\u001b[39m     )\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df.columns, pd.MultiIndex):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# MLP 학습할 데이터셋 구축 (ORIGINAL)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from CNN_PY import cnn_model\n",
    "from LSTM_PY import lstm_model\n",
    "import time\n",
    "\n",
    "# 파일 경로\n",
    "symbols_file = 'completed_symbols_h5.txt'\n",
    "output_csv = 'MLP_sp500_temp.csv'\n",
    "\n",
    "# 기간\n",
    "start_date = '2024-01-01'\n",
    "end_date = '2025-01-01'\n",
    "\n",
    "# 초기 CSV 파일 존재 확인 및 중복 키 확보\n",
    "if os.path.exists(output_csv):\n",
    "    existing_df = pd.read_csv(output_csv)\n",
    "    existing_keys = set(existing_df.apply(lambda x: f\"{x['Symbol']}_{x['Date']}\", axis=1))\n",
    "else:\n",
    "    # 파일 없으면 헤더 생성\n",
    "    with open(output_csv, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Symbol', 'Date', 'Close', 'Label', 'Prob', 'Actual_Close'])\n",
    "    existing_keys = set()\n",
    "\n",
    "# 종목 리스트 로드\n",
    "with open(symbols_file, 'r') as f:\n",
    "    symbols = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "# 종목별 처리\n",
    "for symbol in symbols:\n",
    "    print(f\"\\n### {symbol} 처리 시작 ###\")\n",
    "    try:\n",
    "        # 한번만 데이터 다운로드 (전체 거래일 확보)\n",
    "        df = yf.download(symbol, start=start_date, end=end_date, interval='1d', progress=False)\n",
    "        if df.empty:\n",
    "            print(f\"{symbol}: 기간 내 데이터 없음\")\n",
    "            continue\n",
    "        \n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df.columns = [col[0] for col in df.columns]\n",
    "\n",
    "        df = df.dropna(subset=['Close']).copy()\n",
    "        df['Date_Str'] = df.index.strftime('%Y-%m-%d')\n",
    "\n",
    "        # 거래일별 처리\n",
    "        for idx, row in df.iterrows():\n",
    "            date = row['Date_Str']\n",
    "            key = f\"{symbol}_{date}\"\n",
    "\n",
    "            if key in existing_keys:\n",
    "                print(f\"{symbol} {date}: 이미 존재, 스킵\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # LSTM 예측 Close\n",
    "                close = lstm_model(date, symbol)\n",
    "                if close is None:\n",
    "                    print(f\"{symbol} {date}: LSTM 예측 실패\")\n",
    "                    continue\n",
    "\n",
    "                # CNN 예측 Label, Prob\n",
    "                label, prob = cnn_model(date, symbol)\n",
    "                if label is None or prob is None:\n",
    "                    print(f\"{symbol} {date}: CNN 예측 실패\")\n",
    "                    continue\n",
    "\n",
    "                # 실제 종가 (다운로드된 df에서 사용)\n",
    "                actual_close = row['Close']\n",
    "\n",
    "                print(f\"{symbol} | {date} | Close(LSTM): {close:.2f} | Label: {label} | Prob: {prob:.4f} | Actual_Close: {actual_close:.2f}\")\n",
    "\n",
    "                # CSV 기록\n",
    "                with open(output_csv, 'a', newline='') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    writer.writerow([symbol, date, close, label, prob, actual_close])\n",
    "\n",
    "                existing_keys.add(key)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"{symbol} {date} 처리 오류: {e}\")\n",
    "\n",
    "            time.sleep(0.5)  # 딜레이 (yfinance는 이미 다운로드됨)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{symbol} 전체 처리 오류: {e}\")\n",
    "\n",
    "print(\"\\n### 전체 완료 ###\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6574852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:776: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "c:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:793: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "c:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 255666.9844 - mae: 205.4474 - val_loss: 256762.9531 - val_mae: 204.3289\n",
      "Epoch 2/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 655us/step - loss: 249867.2812 - mae: 201.1551 - val_loss: 254814.1562 - val_mae: 199.5109\n",
      "Epoch 3/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 589us/step - loss: 250188.3594 - mae: 194.4481 - val_loss: 252923.3594 - val_mae: 194.8001\n",
      "Epoch 4/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 618us/step - loss: 267611.2500 - mae: 192.6266 - val_loss: 251086.3438 - val_mae: 190.2311\n",
      "Epoch 5/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 659us/step - loss: 246117.8906 - mae: 187.2902 - val_loss: 249303.1875 - val_mae: 185.8461\n",
      "Epoch 6/50\n",
      "\u001b[1m  93/5625\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 625us/step - loss: 265030.0625 - mae: 179.5308"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m mlp.compile(optimizer=\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m, loss=\u001b[33m'\u001b[39m\u001b[33mmse\u001b[39m\u001b[33m'\u001b[39m, metrics=[\u001b[33m'\u001b[39m\u001b[33mmae\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# 6. Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mmlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# 7. Save model & scaler\u001b[39;00m\n\u001b[32m     37\u001b[39m mlp.save(\u001b[33m'\u001b[39m\u001b[33mMLP_model.h5\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1681\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1682\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1683\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1684\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1685\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1686\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1687\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1688\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1691\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1692\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1693\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1697\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1698\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "import joblib\n",
    "\n",
    "# 1. Load dataset (MLP_sp500.csv)\n",
    "df = pd.read_csv('MLP_sp500.csv')\n",
    "\n",
    "# 2. Ensure actual close (y) is present\n",
    "features = ['Close', 'Label', 'Prob']\n",
    "X = df[features].values\n",
    "y = df['Actual_Close'].values  # 반드시 'Actual_Close' 열이 있어야 함\n",
    "\n",
    "# 3. Scale inputs (입력만 Scaling, y는 그대로)\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 4. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Build MLP model (Regression)\n",
    "mlp = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)  # 종가 예측\n",
    "])\n",
    "\n",
    "mlp.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# 6. Train\n",
    "mlp.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.1)\n",
    "\n",
    "# 7. Save model & scaler\n",
    "mlp.save('MLP_model.h5')\n",
    "joblib.dump(scaler, 'MLP_scaler.joblib')\n",
    "\n",
    "# 8. Evaluate\n",
    "loss, mae = mlp.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss (MSE): {loss:.4f}, MAE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mehtod 1\n",
    "\n",
    "\n",
    "# # 1. 모델과 스케일러 불러오기\n",
    "# # LSTM\n",
    "# lstm_model = keras.layers.TFSMLayer('LSTM_MODEL_TF_GPU/AOS', call_endpoint='serving_default')\n",
    "# lstm_scaler = joblib.load('LSTM_MODEL_TF_GPU/AOS_scaler.joblib')\n",
    "# # CNN\n",
    "# cnn_model = tf.keras.models.load_model.load_model('pattern_classification_model.h5')\n",
    "\n",
    "# # 2. 메타 입력 생성\n",
    "# lstm_out = lstm_model.output  # shape: (None, 1)\n",
    "# cnn_out = cnn_model.layers[-2].output  # 마지막 레이어 직전 추출 (3D 특징 맵)\n",
    "# cnn_flatten = keras.layers.Flatten()(cnn_out)  # shape: (None, 2048)\n",
    "\n",
    "# # 3. 결합 및 MLP 처리\n",
    "# combined = Concatenate()([lstm_out, cnn_flatten])\n",
    "# x = keras.layers.Dense(64, activation='relu')(combined)\n",
    "# x = keras.layers.Dense(32, activation='relu')(x)\n",
    "# final_output = keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "# # 4. 최종 모델 컴파일\n",
    "# meta_model = keras.layers.Model(inputs=[lstm_model.input, cnn_model.input], outputs=final_output)\n",
    "# meta_model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b0c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 2\n",
    "\n",
    "\n",
    "# # LSTM 모델 및 스케일러 불러오기 (학습 가능한 형태로 재구성 필요할 수 있음)\n",
    "# lstm_model_loaded = keras.models.load_model('LSTM_MODEL_TF_GPU/AOS') # SavedModel 형식에 따라 로드 방식이 다를 수 있음\n",
    "# lstm_scaler = joblib.load('LSTM_MODEL_TF_GPU/AOS_scaler.joblib')\n",
    "\n",
    "# # CNN 모델 불러오기\n",
    "# cnn_model_loaded = tf.keras.models.load_model('pattern_classification_model.h5')\n",
    "\n",
    "# # Meta-Model 학습 데이터 생성 함수 (가정)\n",
    "# def generate_multimodal_data(symbols, start_date, end_date, n_lstm_days=50, n_cnn_candles=20):\n",
    "#     lstm_predictions = []\n",
    "#     cnn_predictions = []\n",
    "#     actual_close_changes = [] # 예시: 다음날 종가 변화율\n",
    "#     # ... (데이터 생성 로직 구현: 각 종목, 날짜별 LSTM 예측, CNN 예측, 실제 종가 변화 수집)\n",
    "#     # 이 함수는 실제 데이터와 모델 예측을 기반으로 학습 데이터를 생성해야 합니다.\n",
    "#     # 예시로 임의의 데이터를 생성합니다.\n",
    "#     num_samples = 1000\n",
    "#     lstm_predictions = np.random.rand(num_samples, 1) * 200 # 임의의 종가 예측\n",
    "#     cnn_probabilities = np.random.rand(num_samples, 3) # 임의의 [drop, neutral, surge] 확률\n",
    "#     actual_close_changes = np.random.randn(num_samples, 1) * 0.05 # 임의의 종가 변화율\n",
    "#     return np.concatenate([lstm_predictions, cnn_probabilities], axis=1), actual_close_changes\n",
    "\n",
    "# # Meta-Model 구축\n",
    "# def build_meta_model(lstm_output_dim, cnn_output_dim):\n",
    "#     lstm_input = Input(shape=(lstm_output_dim,))\n",
    "#     cnn_input = Input(shape=(cnn_output_dim,))\n",
    "#     merged = concatenate([lstm_input, cnn_input])\n",
    "#     dense1 = Dense(64, activation='relu')(merged)\n",
    "#     dense2 = Dense(32, activation='relu')(dense1)\n",
    "#     output = Dense(1)(dense2) # 최종 종가 변화율 예측\n",
    "#     meta_model = Model(inputs=[lstm_input, cnn_input], outputs=output)\n",
    "#     return meta_model\n",
    "\n",
    "# # 학습 데이터 생성 (실제 데이터 기반으로 구현 필요)\n",
    "# symbols = ['AAPL', 'MSFT']\n",
    "# start_date = '2020-01-01'\n",
    "# end_date = '2024-12-31'\n",
    "# X_meta, y_meta = generate_multimodal_data(symbols, start_date, end_date)\n",
    "\n",
    "# # Meta-Model 입력 형태 정의\n",
    "# lstm_output_dim = 1 # LSTM 예측 종가\n",
    "# cnn_output_dim = 3 # CNN 예측 확률 (drop, neutral, surge)\n",
    "\n",
    "# # Meta-Model 구축\n",
    "# meta_model = build_meta_model(lstm_output_dim, cnn_output_dim)\n",
    "\n",
    "# # Meta-Model 컴파일\n",
    "# meta_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# # 학습 데이터 분할\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_meta, y_meta, test_size=0.2, random_state=42)\n",
    "# lstm_input_train = X_train[:, :lstm_output_dim]\n",
    "# cnn_input_train = X_train[:, lstm_output_dim:]\n",
    "# lstm_input_test = X_test[:, :lstm_output_dim]\n",
    "# cnn_input_test = X_test[:, lstm_output_dim:]\n",
    "\n",
    "# # Meta-Model 학습\n",
    "# meta_model.fit([lstm_input_train, cnn_input_train], y_train, epochs=10, batch_size=32, validation_data=([lstm_input_test, cnn_input_test], y_test))\n",
    "\n",
    "# # Meta-Model 평가\n",
    "# loss = meta_model.evaluate([lstm_input_test, cnn_input_test], y_test)\n",
    "# print(f\"Meta-Model Test Loss: {loss}\")\n",
    "\n",
    "# # Meta-Model 예측\n",
    "# predictions = meta_model.predict([lstm_input_test, cnn_input_test])\n",
    "# print(\"Meta-Model Predictions:\")\n",
    "# print(predictions[:5])\n",
    "# print(\"Actual Changes:\")\n",
    "# print(y_test[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
